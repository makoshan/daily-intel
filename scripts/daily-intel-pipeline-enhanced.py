#!/usr/bin/env python3
"""
Daily Intel Pipeline - Enhanced Version
æ•´åˆäº†å¤šå¹³å°æ•°æ®æŠ“å–å’Œ AI å†…å®¹å¢å¼º
"""

import os
import sys
import subprocess
from datetime import datetime
from pathlib import Path

# æ·»åŠ  src ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))

from dotenv import load_dotenv
load_dotenv()

# å¯¼å…¥è„šæœ¬æ¨¡å—
from rss_fetcher import fetch_single_feed
from content_enhancer import ContentEnhancer
from hn_comment_analyzer import HNCommentAnalyzer

# å¯¼å…¥ src æ¨¡å—
try:
    from fetcher import IntelAggregator
    USE_ENHANCED_FETCHER = True
except ImportError:
    print("âš ï¸  Warning: Could not import enhanced fetcher from src/")
    USE_ENHANCED_FETCHER = False


class DailyIntelPipeline:
    """Daily Intel å®Œæ•´è‡ªåŠ¨åŒ–æµç¨‹ - å¢å¼ºç‰ˆ"""
    
    def __init__(self):
        self.api_key = os.getenv("OPENAI_API_KEY")
        self.api_base = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
        
        if not self.api_key:
            raise ValueError("OPENAI_API_KEY not set! Please set it in .env file")
        
        self.enhancer = ContentEnhancer(self.api_key, self.api_base)
        self.hn_analyzer = HNCommentAnalyzer(self.api_key, self.api_base)
        
        self.date = datetime.now()
        self.date_str = self.date.strftime("%Y-%m-%d")
        self.date_display = self.date.strftime("%Yå¹´%mæœˆ%dæ—¥")
        
        # åˆå§‹åŒ–å¢å¼ºå‹æŠ“å–å™¨
        if USE_ENHANCED_FETCHER:
            self.aggregator = IntelAggregator()
        else:
            self.aggregator = None
    
    def run_full_pipeline(self) -> str:
        """è¿è¡Œå®Œæ•´æµç¨‹"""
        print("=" * 70)
        print(f"Daily Intel è‡ªåŠ¨åŒ–æµç¨‹ - {self.date_display}")
        print("=" * 70)
        
        # Step 1: æŠ“å–æ•°æ®
        print("\n[1/5] æŠ“å–æ•°æ®æº...")
        data = self.fetch_all_sources()
        
        # Step 2: AI å†…å®¹å¢å¼º
        print("\n[2/5] AI å†…å®¹å¢å¼º...")
        enhanced = self.enhance_content(data)
        
        # Step 3: ç”Ÿæˆæ–‡ç« 
        print("\n[3/5] ç”Ÿæˆ Markdown æ–‡ç« ...")
        markdown = self.generate_markdown(enhanced)
        
        # Step 4: ä¿å­˜æ–‡ä»¶
        print("\n[4/5] ä¿å­˜æ–‡ç« ...")
        filepath = self.save_article(markdown)
        
        # Step 5: Git æäº¤
        print("\n[5/5] Git æäº¤...")
        self.git_commit_push(filepath)
        
        print("\n" + "=" * 70)
        print("âœ… æµç¨‹å®Œæˆ!")
        print(f"æ–‡ç« åœ°å€: https://makoshan.github.io/daily-intel/{self.date.strftime('%Y%m%d')}.html")
        print("=" * 70)
        
        return filepath
    
    def fetch_all_sources(self) -> dict:
        """æŠ“å–æ‰€æœ‰æ•°æ®æº - ä½¿ç”¨å¢å¼ºå‹æŠ“å–å™¨"""
        data = {
            "rss": {},
            "hn_top": [],
            "hn_discussions": [],
            "platforms": {}
        }
        
        if USE_ENHANCED_FETCHER and self.aggregator:
            # ä½¿ç”¨å¢å¼ºå‹å¤šå¹³å°æŠ“å–å™¨
            print("  ä½¿ç”¨å¢å¼ºå‹å¤šå¹³å°æŠ“å–å™¨...")
            try:
                platform_data = self.aggregator.fetch_all()
                data["platforms"] = platform_data
                
                # ç»Ÿè®¡
                total = sum(len(items) for items in platform_data.values())
                print(f"\n  âœ“ å¢å¼ºå‹æŠ“å–å®Œæˆ: {total} ç¯‡æ–‡ç« ")
                for platform, items in platform_data.items():
                    if items:
                        print(f"    - {platform}: {len(items)} ç¯‡")
            except Exception as e:
                print(f"  âœ— å¢å¼ºå‹æŠ“å–å¤±è´¥: {e}")
                print(f"  å›é€€åˆ°åŸºç¡€ RSS æŠ“å–...")
        
        # RSS æºï¼ˆä½œä¸ºè¡¥å……æˆ–å¤‡ç”¨ï¼‰
        print("\n  æŠ“å– RSS æº...")
        try:
            print("    - News Hacker RSS...")
            data["rss"]["newshacker"] = fetch_single_feed("https://api.newshacker.me/rss", 5)
        except Exception as e:
            print(f"      é”™è¯¯: {e}")
        
        try:
            print("    - Hacker Podcast RSS...")
            data["rss"]["hacker_podcast"] = fetch_single_feed("https://hacker-podcast.agi.li/rss.xml", 3)
        except Exception as e:
            print(f"      é”™è¯¯: {e}")
        
        # HN Top Stories
        print("\n  - æŠ“å– HN Top Stories...")
        try:
            top_ids = self.hn_analyzer.fetch_top_stories(limit=10)
            
            for story_id in top_ids[:5]:
                try:
                    story = self.hn_analyzer.fetch_item(story_id)
                    if story and story.get("type") == "story":
                        data["hn_top"].append({
                            "id": story_id,
                            "title": story.get("title", ""),
                            "url": story.get("url", ""),
                            "score": story.get("score", 0),
                            "descendants": story.get("descendants", 0)
                        })
                except Exception as e:
                    print(f"    é”™è¯¯: {e}")
        except Exception as e:
            print(f"  HN æŠ“å–å¤±è´¥: {e}")
        
        # HN è¯„è®ºåˆ†æ
        print("\n  - åˆ†æ HN è¯„è®º...")
        try:
            sorted_by_comments = sorted(data["hn_top"], 
                                       key=lambda x: x.get("descendants", 0), 
                                       reverse=True)
            
            for story in sorted_by_comments[:2]:
                try:
                    story_data = self.hn_analyzer.fetch_story_with_comments(story["id"])
                    if story_data.get("comments"):
                        analysis = self.hn_analyzer.analyze_comments_with_ai(story_data)
                        data["hn_discussions"].append(analysis)
                except Exception as e:
                    print(f"    åˆ†æå¤±è´¥: {e}")
        except Exception as e:
            print(f"  HN è¯„è®ºåˆ†æå¤±è´¥: {e}")
        
        # æ‰“å°ç»Ÿè®¡
        print(f"\n  æŠ“å–ç»“æœæ±‡æ€»:")
        print(f"    - RSS æ–‡ç« : {sum(len(v) for v in data['rss'].values())} ç¯‡")
        print(f"    - HN Top: {len(data['hn_top'])} ç¯‡")
        print(f"    - HN è¯„è®ºåˆ†æ: {len(data['hn_discussions'])} ç¯‡")
        if data["platforms"]:
            print(f"    - å…¶ä»–å¹³å°: {sum(len(v) for v in data['platforms'].values())} ç¯‡")
        
        return data
    
    def enhance_content(self, data: dict) -> dict:
        """AI å†…å®¹å¢å¼º"""
        enhanced = {
            "articles": [],
            "hn_discussions": data.get("hn_discussions", [])
        }
        
        # æ”¶é›†æ‰€æœ‰æ–‡ç« 
        all_articles = []
        
        # ä»å¹³å°æ•°æ®æ”¶é›†
        for platform, items in data.get("platforms", {}).items():
            for item in items[:3]:  # æ¯ä¸ªå¹³å°å–å‰3ç¯‡
                all_articles.append({
                    "title": item.get("title", ""),
                    "link": item.get("url", ""),
                    "description": item.get("description", ""),
                    "source": item.get("platform", platform)
                })
        
        # ä» RSS æ”¶é›†
        for source, articles in data.get("rss", {}).items():
            for article in articles[:3]:
                all_articles.append({
                    **article,
                    "source": source
                })
        
        # ä» HN Top Stories æ”¶é›†
        for story in data.get("hn_top", [])[:3]:
            all_articles.append({
                "title": story["title"],
                "link": f"https://news.ycombinator.com/item?id={story['id']}",
                "description": f"HN Top Story - {story['score']} points, {story['descendants']} comments",
                "source": "Hacker News"
            })
        
        # AI å¢å¼ºï¼ˆé™åˆ¶æ•°é‡ä»¥æ§åˆ¶æˆæœ¬ï¼‰
        max_articles = int(os.getenv("MAX_ARTICLES_PER_RUN", "10"))
        for i, article in enumerate(all_articles[:max_articles], 1):
            print(f"  å¢å¼ºæ–‡ç«  {i}/{min(len(all_articles), max_articles)}: {article['title'][:40]}...")
            try:
                enhanced_article = self.enhancer.enhance_article(
                    article["title"],
                    article.get("description", ""),
                    article["link"]
                )
                enhanced_article["source"] = article.get("source", "Unknown")
                enhanced["articles"].append(enhanced_article)
            except Exception as e:
                print(f"    å¤±è´¥: {e}")
        
        print(f"\n  âœ“ å¢å¼ºå®Œæˆ: {len(enhanced['articles'])} ç¯‡")
        return enhanced
    
    def generate_markdown(self, enhanced: dict) -> str:
        """ç”Ÿæˆ Markdown æ–‡ç« """
        
        # æå–æ ‡ç­¾
        all_tags = ["ç§‘æŠ€", "AI"]
        for article in enhanced.get("articles", []):
            if article.get("status") == "success":
                tags = self.enhancer.extract_tags(
                    article["title"], 
                    article.get("enhanced_analysis", "")
                )
                all_tags.extend([t.replace("#", "") for t in tags])
        
        # å»é‡å¹¶é™åˆ¶
        unique_tags = list(dict.fromkeys(all_tags))[:5]
        tags_str = ", ".join([f'"{t}"' for t in unique_tags])
        
        md = f"""---
layout: post
title: "æ¯æ—¥ç§‘æŠ€æƒ…æŠ¥ - {self.date_display}"
date: {self.date_str} 08:00:00 +0800
categories: daily
tags: [{tags_str}]
permalink: /{self.date.strftime('%Y%m%d')}.html
---

# æ¯æ—¥ç§‘æŠ€æƒ…æŠ¥ | {self.date_display}

> ä¸åªæ˜¯èµ„è®¯ï¼Œæ›´æœ‰æŠ€æœ¯è¶‹åŠ¿ä¸å¤šå…ƒè§‚ç‚¹çš„ç¢°æ’
> 
> è‡ªåŠ¨ç”Ÿæˆ | AI å¢å¼º | HN è¯„è®ºåˆ†æ | å¤šå¹³å°èšåˆ

---

## ğŸ”¥ ä»Šæ—¥çƒ­ç‚¹

"""
        
        # AI å¢å¼ºçš„æ–‡ç« 
        for i, article in enumerate(enhanced.get("articles", [])[:5], 1):
            if article.get("status") == "success":
                tags = self.enhancer.extract_tags(article["title"], article.get("enhanced_analysis", ""))
                tags_line = " ".join(tags[:3])
                
                md += f"""### {i}. {article['title']}

{tags_line}

{article['enhanced_analysis']}

**æ¥æº**: {article['source']}

---

"""
        
        # HN è®¨è®º
        if enhanced.get("hn_discussions"):
            md += """## ğŸ’¬ HN ç¤¾åŒºè§‚ç‚¹

"""
            for disc in enhanced["hn_discussions"]:
                story = disc.get("story", {})
                md += f"""### {story.get('title', '')}

{disc.get('comment_count', 0)} æ¡è¯„è®º | {story.get('score', 0)} åˆ†

{disc.get('analysis', '')}

[æŸ¥çœ‹è®¨è®º](https://news.ycombinator.com/item?id={story.get('id', '')})

---

"""
        
        # æ•°æ®æ¦‚è§ˆ
        md += f"""## ğŸ“Š æ•°æ®æ¦‚è§ˆ

| æ¥æº | æ•°é‡ | é‡ç‚¹ |
|------|------|------|
| å¤šå¹³å°èšåˆ | {len(enhanced.get('articles', []))} ç¯‡ | AIã€æŠ€æœ¯è¶‹åŠ¿ã€äº§å“ |
| HN è®¨è®º | {len(enhanced.get('hn_discussions', []))} ç¯‡ | å¤šå…ƒè§‚ç‚¹ã€ç¤¾åŒºçƒ­ç‚¹ |

---

*è‡ªåŠ¨ç”Ÿæˆäº {self.date.strftime('%Y-%m-%d %H:%M')} | Powered by AI*
"""
        
        return md
    
    def save_article(self, markdown: str) -> str:
        """ä¿å­˜æ–‡ç« åˆ° _posts ç›®å½•"""
        # ç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•
        script_dir = Path(__file__).parent
        project_dir = script_dir.parent
        posts_dir = project_dir / "_posts"
        
        # åˆ›å»º _posts ç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        posts_dir.mkdir(exist_ok=True)
        
        filename = posts_dir / f"{self.date_str}-daily-intel.md"
        
        with open(filename, "w", encoding="utf-8") as f:
            f.write(markdown)
        
        print(f"  å·²ä¿å­˜: {filename}")
        return str(filename)
    
    def git_commit_push(self, filepath: str):
        """Git æäº¤å¹¶æ¨é€"""
        try:
            # è·å–é¡¹ç›®æ ¹ç›®å½•
            script_dir = Path(__file__).parent
            project_dir = script_dir.parent
            
            # æ£€æŸ¥ git çŠ¶æ€
            result = subprocess.run(
                ["git", "status", "--short"],
                capture_output=True, text=True, cwd=str(project_dir)
            )
            
            if not result.stdout.strip():
                print("  æ— å˜æ›´ï¼Œè·³è¿‡æäº¤")
                return
            
            # æ·»åŠ æ–‡ä»¶ï¼ˆä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼‰
            rel_path = Path(filepath).relative_to(project_dir)
            subprocess.run(["git", "add", str(rel_path)], check=True, cwd=str(project_dir))
            
            # æäº¤
            commit_msg = f"Add Daily Intel - {self.date_display}"
            subprocess.run(["git", "commit", "-m", commit_msg], check=True, cwd=str(project_dir))
            
            # æ¨é€
            subprocess.run(["git", "push", "origin", "master"], check=True, cwd=str(project_dir))
            
            print("  âœ“ Git æäº¤å¹¶æ¨é€æˆåŠŸ")
            
        except subprocess.CalledProcessError as e:
            print(f"  âœ— Git æ“ä½œå¤±è´¥: {e}")
        except Exception as e:
            print(f"  âœ— é”™è¯¯: {e}")


def main():
    """ä¸»å‡½æ•°"""
    try:
        # æ£€æŸ¥ç¯å¢ƒå˜é‡
        if not os.getenv("OPENAI_API_KEY"):
            print("\né”™è¯¯: æœªè®¾ç½® OPENAI_API_KEY")
            print("è¯·åˆ›å»º scripts/.env æ–‡ä»¶å¹¶æ·»åŠ :")
            print("OPENAI_API_KEY=your-api-key-here")
            sys.exit(1)
        
        pipeline = DailyIntelPipeline()
        pipeline.run_full_pipeline()
        
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
