#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Daily Newsletter .page ç”Ÿæˆå™¨

ç›´æ¥ä» RSS æºç”Ÿæˆ Hakyll .page æ ¼å¼

ç”¨æ³•:
    python generator.py              # ç”Ÿæˆä»Šå¤©çš„ Newsletter
    python generator.py 2026-02-09   # ç”ŸæˆæŒ‡å®šæ—¥æœŸçš„ Newsletter
"""

import os
import sys
import io
import re
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Set, Tuple
from collections import defaultdict

# ä¿®å¤ Windows æ§åˆ¶å°ç¼–ç é—®é¢˜
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# å¯¼å…¥ RSS æŠ“å–æ¨¡å—
try:
    from rss_fetcher import RSSFetcher
    HAS_RSS = True
except ImportError:
    print("[è­¦å‘Š] æœªæ‰¾åˆ° rss_fetcher æ¨¡å—")
    HAS_RSS = False


class NewsletterGenerator:
    """Newsletter .page æ ¼å¼ç”Ÿæˆå™¨"""
    
    def __init__(self, output_dir: str = "newsletter"):
        # æ ¹ç›®å½•æ˜¯é¡¹ç›®æ ¹ç›®å½•ï¼ˆscripts/newsletterçš„ä¸Šä¸¤çº§ï¼‰
        script_dir = Path(__file__).parent
        project_root = script_dir.parent.parent
        self.output_dir = project_root / output_dir
        self.output_dir.mkdir(exist_ok=True)
    
    def fetch_rss_data(self) -> Dict:
        """ä» RSS æºè·å–æ•°æ®"""
        if HAS_RSS:
            fetcher = RSSFetcher()
            print("[*] ä» RSS æºè·å–æ•°æ®...")
            rss_data = fetcher.fetch_all_sources(limit_per_source=10)
            return rss_data
        else:
            return {}
    
    def extract_topics(self, text: str) -> Set[str]:
        """ä»æ–‡æœ¬æå–ä¸»é¢˜æ ‡ç­¾"""
        topics = set()
        
        # é¢„å®šä¹‰çš„ä¸»é¢˜å…³é”®è¯
        topic_keywords = {
            'AI': ['ai', 'gpt', 'llm', 'machine learning', 'æœºå™¨å­¦ä¹ ', 'äººå·¥æ™ºèƒ½', 'openai', 'claude'],
            'Web3': ['blockchain', 'crypto', 'web3', 'defi', 'nft', 'åŒºå—é“¾', 'åŠ å¯†'],
            'Security': ['security', 'hack', 'vulnerability', 'å®‰å…¨', 'æ¼æ´'],
            'Programming': ['programming', 'code', 'developer', 'ç¼–ç¨‹', 'å¼€å‘', 'github'],
            'Rust': ['rust', 'cargo'],
            'Python': ['python', 'pip'],
            'Haskell': ['haskell', 'cabal'],
            'Cloud': ['cloud', 'aws', 'azure', 'äº‘è®¡ç®—'],
            'Startup': ['startup', 'founder', 'vc', 'åˆ›ä¸š'],
        }
        
        text_lower = text.lower()
        for topic, keywords in topic_keywords.items():
            for keyword in keywords:
                if keyword.lower() in text_lower:
                    topics.add(topic)
                    break
        
        return topics
    
    def generate_newsletter_page(self, rss_data: Dict, date_str: str) -> Tuple[str, Set[str]]:
        """ç”Ÿæˆ .page æ ¼å¼çš„ Newsletter"""
        
        # æ ¼å¼åŒ–æ—¥æœŸ
        try:
            dt = datetime.strptime(date_str, '%Y-%m-%d')
            created = dt.strftime('%d %b %Y')  # 09 Feb 2026
            formatted_date = dt.strftime('%Yå¹´%mæœˆ%dæ—¥')
            weekday = ['å‘¨ä¸€', 'å‘¨äºŒ', 'å‘¨ä¸‰', 'å‘¨å››', 'å‘¨äº”', 'å‘¨å…­', 'å‘¨æ—¥'][dt.weekday()]
        except:
            created = date_str
            formatted_date = date_str
            weekday = ""
        
        # æ”¶é›†æ–‡ç« ä¿¡æ¯å’Œä¸»é¢˜
        all_topics = set()
        articles = []
        
        for source_key, source_data in rss_data.items():
            if not source_data.get("articles"):
                continue
            
            for article in source_data["articles"][:10]:
                title = article.get("title", "")
                description = article.get("description", "")
                link = article.get("link", "")
                
                # æå–ä¸»é¢˜
                topics = self.extract_topics(title + " " + description)
                all_topics.update(topics)
                
                articles.append({
                    'title': title,
                    'description': description,
                    'link': link,
                    'source': source_data['name'],
                    'category': source_data['category'],
                    'topics': topics
                })
        
        # ç”Ÿæˆæ ‡é¢˜å’Œæè¿°
        title = f"æ¯æ—¥ç§‘æŠ€æƒ…æŠ¥ - {formatted_date}"
        topics_str = ", ".join(sorted(all_topics)) if all_topics else "tech, newsletter"
        
        if all_topics:
            topics_preview = ", ".join(list(sorted(all_topics))[:5])
            description = f"æ¯æ—¥ç§‘æŠ€æƒ…æŠ¥ã€‚æœ¬æœŸå…³æ³¨: {topics_preview}"
        else:
            description = "æ¯æ—¥ç§‘æŠ€æƒ…æŠ¥ï¼ŒåŒ…å« AIã€Web3ã€å®‰å…¨ã€å¼€å‘å·¥å…·ç­‰é¢†åŸŸçš„ç²¾é€‰èµ„è®¯"
        
        # ç”Ÿæˆ .page å†…å®¹
        page_content = f"""---
title: {title}
description: "{description}"
tags: newsletter, {topics_str}
created: {created}
status: finished
belief: log
importance: 5
...

# ğŸ“Š æ¯æ—¥ç§‘æŠ€æƒ…æŠ¥ | {formatted_date} {weekday}

> ä¸åªæ˜¯èµ„è®¯ï¼Œæ›´æœ‰æŠ€æœ¯è¶‹åŠ¿ä¸å¤šå…ƒè§‚ç‚¹çš„ç¢°æ’

"""
        
        # æ·»åŠ æœ¬æœŸä¸»é¢˜æ ‡ç­¾
        if all_topics:
            page_content += f"**æœ¬æœŸä¸»é¢˜**: {' Â· '.join(f'#{t}' for t in sorted(all_topics))}\n\n"
        
        page_content += "---\n\n## ğŸ”¥ ä»Šæ—¥é‡ç‚¹\n\n"
        
        # æ·»åŠ æ–‡ç« 
        for i, article in enumerate(articles[:10], 1):
            # æ¸…ç†æè¿°
            desc = re.sub(r'<[^>]+>', '', article['description'])
            desc = desc[:300] + "..." if len(desc) > 300 else desc
            
            # ä¸»é¢˜æ ‡ç­¾
            topic_tags = ' '.join(f'#{t}' for t in sorted(article['topics'])) if article['topics'] else ''
            
            page_content += f"""
### {i}. {article['title']}

{topic_tags}

**æ¥æº**: [{article['source']}]() Â· **ç±»åˆ«**: {article['category']}

{desc}

ğŸ”— [æŸ¥çœ‹åŸæ–‡]({article['link']})

---

"""
        
        # æ·»åŠ æ•°æ®æºä¿¡æ¯
        page_content += "\n## ğŸ“¡ æ•°æ®æ¥æº\n\næœ¬æœŸå†…å®¹èšåˆè‡ªä»¥ä¸‹å¹³å°ï¼š\n\n"
        
        for source_key, source_data in rss_data.items():
            article_count = len([a for a in articles if a['source'] == source_data['name']])
            if article_count > 0:
                page_content += f"- **[{source_data['name']}]()** ({source_data['category']}) - {article_count} æ¡\n"
        
        # æ·»åŠ å…ƒä¿¡æ¯
        page_content += f"""

---

## ğŸ“® Newsletter ä¿¡æ¯

**æ›´æ–°é¢‘ç‡**: æ¯æ—¥ 08:00 (åŒ—äº¬æ—¶é—´)

**æ¶µç›–é¢†åŸŸ**: ğŸ¤– AI Â· ğŸ” å®‰å…¨ Â· ğŸ’» å¼€å‘ Â· ğŸš€ åˆ›ä¸š Â· ğŸŒ Web3

---

## ğŸ§­ å¯¼èˆª

- [Newsletter é¦–é¡µ](/newsletter/) - æ‰€æœ‰æœŸåˆŠç´¢å¼•
- [ä¸»é¢˜ç´¢å¼•](/newsletter/topics) - æŒ‰ä¸»é¢˜æµè§ˆ
- [å…³äºæœ¬ç«™](/about) - é¡¹ç›®ç†å¿µ

---

*ç”Ÿæˆäº {created} Â· èšåˆè‡ª {len(rss_data)} ä¸ªä¿¡æ¯æº*
"""
        
        return page_content, all_topics
    
    def save_newsletter(self, content: str, date_str: str) -> Path:
        """ä¿å­˜ Newsletter .page æ–‡ä»¶ï¼ˆæŒ‰å¹´ä»½ç›®å½•ç»„ç»‡ï¼‰"""
        # æå–å¹´ä»½
        year = date_str.split('-')[0]
        year_dir = self.output_dir / year
        year_dir.mkdir(exist_ok=True)
        
        filename = f"newsletter-{date_str}.page"
        filepath = year_dir / filename
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(content)
        
        return filepath
    
    def generate_index_page(self):
        """ç”Ÿæˆ Newsletter é¦–é¡µç´¢å¼•"""
        # æŸ¥æ‰¾æ‰€æœ‰ newsletter .page æ–‡ä»¶
        newsletters = []
        for year_dir in sorted(self.output_dir.glob("*"), reverse=True):
            if year_dir.is_dir() and year_dir.name.isdigit():
                newsletters.extend(year_dir.glob("newsletter-*.page"))
        
        newsletters = sorted(newsletters, reverse=True)
        
        index_content = f"""---
title: Newsletter å½’æ¡£
description: Daily Intel Newsletter æ‰€æœ‰æœŸåˆŠç´¢å¼•
tags: newsletter, index
created: {datetime.now().strftime('%d %b %Y')}
status: finished
belief: log
...

# ğŸ“§ Newsletter å½’æ¡£

> Daily Intel æ¯æ—¥ç§‘æŠ€æƒ…æŠ¥ - æ‰€æœ‰æœŸåˆŠ

å…± {len(newsletters)} æœŸ Newsletter

---

## ğŸ“… æœ€æ–°æœŸåˆŠ

"""
        
        # æœ€æ–° 10 æœŸ
        for nl in newsletters[:10]:
            date_match = re.search(r'newsletter-(\d{4}-\d{2}-\d{2})', nl.name)
            if date_match:
                date = date_match.group(1)
                year = date.split('-')[0]
                index_content += f"- [{date}](/{year}/newsletter-{date})\n"
        
        # æŒ‰å¹´å½’æ¡£
        index_content += "\n## ğŸ“… æŒ‰å¹´æµè§ˆ\n\n"
        
        by_year = defaultdict(int)
        for nl in newsletters:
            date_match = re.search(r'newsletter-(\d{4})-', nl.name)
            if date_match:
                year = date_match.group(1)
                by_year[year] += 1
        
        for year in sorted(by_year.keys(), reverse=True):
            count = by_year[year]
            index_content += f"- **{year}** - {count} æœŸ\n"
        
        index_content += """

---

[ä¸»é¢˜ç´¢å¼• â†’](/newsletter/topics)
"""
        
        # ä¿å­˜é¦–é¡µ
        index_path = self.output_dir / "index.page"
        with open(index_path, 'w', encoding='utf-8') as f:
            f.write(index_content)
        
        print(f"[*] Newsletter é¦–é¡µå·²æ›´æ–°: {index_path}")
        return index_path
    
    def generate(self, date_str: str = None) -> Path:
        """ç”Ÿæˆ Newsletter"""
        if date_str is None:
            date_str = datetime.now().strftime("%Y-%m-%d")
        
        print(f"[*] ç”Ÿæˆ {date_str} çš„ Newsletter...")
        
        # 1. è·å– RSS æ•°æ®
        rss_data = self.fetch_rss_data()
        
        if not rss_data:
            print("[X] æœªè·å–åˆ° RSS æ•°æ®")
            return None
        
        # 2. ç”Ÿæˆ .page å†…å®¹
        print(f"[*] ç”Ÿæˆ .page æ ¼å¼...")
        newsletter_content, topics = self.generate_newsletter_page(rss_data, date_str)
        
        # 3. ä¿å­˜æ–‡ä»¶
        filepath = self.save_newsletter(newsletter_content, date_str)
        print(f"[OK] Newsletter å·²ä¿å­˜: {filepath}")
        
        # 4. æ›´æ–°ç´¢å¼•é¡µé¢
        print(f"[*] æ›´æ–°ç´¢å¼•é¡µé¢...")
        self.generate_index_page()
        
        # 5. æ˜¾ç¤ºä¸»é¢˜ä¿¡æ¯
        if topics:
            print(f"[*] æœ¬æœŸä¸»é¢˜: {', '.join(sorted(topics))}")
        
        return filepath


def main():
    """ä¸»å‡½æ•°"""
    print("="*60)
    print("Daily Newsletter ç”Ÿæˆå™¨")
    print("="*60)
    print()
    
    # æ£€æŸ¥ä¾èµ–
    if not HAS_RSS:
        print("[X] é”™è¯¯: RSS æ¨¡å—æœªå®‰è£…")
        print("è¯·å®‰è£…: pip install feedparser")
        sys.exit(1)
    
    # å¤„ç†å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1:
        date_str = sys.argv[1]
    else:
        date_str = None
    
    # ç”Ÿæˆ
    generator = NewsletterGenerator()
    filepath = generator.generate(date_str)
    
    if filepath:
        print()
        print("[OK] å®Œæˆ!")
        print()
        print(f"ç”Ÿæˆçš„æ–‡ä»¶: {filepath}")
        print()
        print("ä¸‹ä¸€æ­¥:")
        print("  1. ç¼–è¯‘: bash scripts/build.sh --skip-convert")
        print("  2. é¢„è§ˆ: http://localhost:8000/newsletter/")


if __name__ == "__main__":
    main()
